{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPb+eXEY82afytUH+emxdgF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moin1306/ai_chatbot/blob/main/copy_of_medichat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MediChat : AI Health Assistant Chatbot**\n",
        "\n",
        "This project developed a creative AI-powered medical chatbot to support users with health-related questions. By fine-tuning a language model on a specialized medical dataset, the chatbot provides accurate and reliable answers tailored to user queries. A streamlined Gradio interface allows users of all skill levels to easily input questions and receive clear, practical responses. Built in Python, the project focuses on efficiency and flexibility, optimizing the model to deliver dependable health guidance in an accessible, user-friendly way, enhancing access to healthcare information.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Pq0ZJ-E0mSrE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Installing Unsloth**\n",
        "\n",
        "Installs unsloth and its latest version from GitHub for model loading and fine-tuning."
      ],
      "metadata": {
        "id": "vDmhPCfxfgU4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unsloth # install unsloth\n",
        "!pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git # Also get the latest version Unsloth!"
      ],
      "metadata": {
        "id": "eu_tnkMe4aMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import Libraries**  \n",
        "Imports libraries for model loading, fine-tuning, and dataset handling."
      ],
      "metadata": {
        "id": "_0Dl4T5NiB4M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step3: Import necessary libraries\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "from trl import SFTTrainer\n",
        "from unsloth import is_bfloat16_supported\n",
        "from huggingface_hub import login\n",
        "from transformers import TrainingArguments\n",
        "from datasets import load_dataset\n",
        "import wandb"
      ],
      "metadata": {
        "id": "0OPC1RC-5-cd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Check Hugging Face Token**   \n",
        "Authenticates with Hugging Face using a token stored in Colab secrets."
      ],
      "metadata": {
        "id": "RBVB66HviNOz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step4: Check HF token\n",
        "from google.colab import userdata\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token)"
      ],
      "metadata": {
        "id": "QiV2rrmy6HVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Check GPU Availability**\n",
        "\n",
        "Verifies CUDA and GPU availability on Colab."
      ],
      "metadata": {
        "id": "Yl-Ss3Hxibqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test if CUDA is available\n",
        "import torch\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"GPU device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")"
      ],
      "metadata": {
        "id": "43w_iFnd6Hfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Pretrained Model**\n",
        "\n",
        "Loads the deepseek-ai/deepseek-llm-7b-chat model with 4-bit quantization (7B Parameters)."
      ],
      "metadata": {
        "id": "AKsBwHPTitI8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step5: Setup pretrained DeepSeek-R1\n",
        "\n",
        "model_name = \"deepseek-ai/deepseek-llm-7b-chat\"\n",
        "max_sequence_length = 2048\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = model_name,\n",
        "    max_seq_length = max_sequence_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    token = hf_token\n",
        ")"
      ],
      "metadata": {
        "id": "lK9Rp43r6Ho9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define System Prompt**\n",
        "\n",
        "Sets up a prompt template for medical question-answering."
      ],
      "metadata": {
        "id": "4ETTAOeWjVPs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step6: Setup system prompt\n",
        "prompt_style = \"\"\"\n",
        "Below is a task description along with additional context provided in the input section. Your goal is to provide a well-reasoned response that effectively addresses the request.\n",
        "\n",
        "Before crafting your answer, take a moment to carefully analyze the question. Develop a clear, step-by-step thought process to ensure your response is both logical and accurate.\n",
        "\n",
        "### Task:\n",
        "You are a medical expert specializing in clinical reasoning, diagnostics, and treatment planning. Answer the medical question below using your advanced knowledge.\n",
        "\n",
        "### Query:\n",
        "{}\n",
        "\n",
        "### Answer:\n",
        "<think>{}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "A6ukIo2V7_l_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run Inference (Pre-Fine-Tuning)**\n",
        "\n",
        "Tests the pretrained model with a medical question about cystometry."
      ],
      "metadata": {
        "id": "Uthw6Of7jhay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step7: Run Inference on the model\n",
        "\n",
        "# Define a test question\n",
        "question = \"\"\"A 61-year-old woman with a long history of involuntary urine loss during activities like coughing or\n",
        "              sneezing but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings,\n",
        "              what would cystometry most likely reveal about her residual volume and detrusor contractions?\"\"\"\n",
        "\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# Tokenize the input\n",
        "inputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# Generate a response\n",
        "outputs = model.generate (\n",
        "    input_ids = inputs.input_ids,\n",
        "    attention_mask = inputs.attention_mask,\n",
        "    max_new_tokens = 1200,\n",
        "    use_cache = True\n",
        ")\n",
        "\n",
        "# Decode the response tokens back to text\n",
        "response = tokenizer.batch_decode(outputs)\n",
        "\n",
        "\n",
        "print(response)\n"
      ],
      "metadata": {
        "id": "5bOmb_UT7_yG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response[0].split(\"### Answer:\")[1])"
      ],
      "metadata": {
        "id": "yXlYCXSBJuIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Medical Dataset**\n",
        "\n",
        "Loads a medical dataset for fine-tuning."
      ],
      "metadata": {
        "id": "tDy28dX6jxRz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step8: Setup fine-tuning\n",
        "\n",
        "# Load Dataset\n",
        "medical_dataset = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\", \"en\", split = \"train[:500]\", trust_remote_code = True)"
      ],
      "metadata": {
        "id": "rP-92kDoJuLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "medical_dataset[1]"
      ],
      "metadata": {
        "id": "5XPV24w-J4c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define EOS Token**\n",
        "\n",
        "Retrieves the modelâ€™s end-of-sequence token."
      ],
      "metadata": {
        "id": "cBpoIibmkCJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EOS_TOKEN = tokenizer.eos_token  # Define EOS_TOKEN which tells the model when to stop generating text during training\n",
        "EOS_TOKEN"
      ],
      "metadata": {
        "id": "fwkTUxMKJ4f0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Training Prompt**\n",
        "Sets up a prompt template for fine-tuning."
      ],
      "metadata": {
        "id": "XcHeGerFkKrT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Finetuning\n",
        "# Updated training prompt style to add </think> tag\n",
        "train_prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context.\n",
        "Write a response that appropriately completes the request.\n",
        "Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
        "\n",
        "### Instruction:\n",
        "You are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning.\n",
        "Please answer the following medical question.\n",
        "\n",
        "### Question:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "<think>\n",
        "{}\n",
        "</think>\n",
        "{}\"\"\"\n",
        "\n"
      ],
      "metadata": {
        "id": "jNUqVodgKFWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocess Dataset**\n",
        "\n",
        "Prepare the data for fine-tuning"
      ],
      "metadata": {
        "id": "TSdREu8dkVq7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the data for fine-tuning\n",
        "\n",
        "def preprocess_input_data(examples):\n",
        "  inputs = examples[\"Question\"]\n",
        "  cots = examples[\"Complex_CoT\"]\n",
        "  outputs = examples[\"Response\"]\n",
        "\n",
        "  texts = []\n",
        "\n",
        "  for input, cot, output in zip(inputs, cots, outputs):\n",
        "    text = train_prompt_style.format(input, cot, output) + EOS_TOKEN\n",
        "    texts.append(text)\n",
        "\n",
        "  return {\n",
        "      \"texts\" : texts,\n",
        "  }"
      ],
      "metadata": {
        "id": "vPOUdCYSKFZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finetune_dataset = medical_dataset.map(preprocess_input_data, batched = True)"
      ],
      "metadata": {
        "id": "Msg01aVNKNCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finetune_dataset[\"texts\"][0]"
      ],
      "metadata": {
        "id": "oOZ8r9zlKNFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Apply LoRA**\n",
        "\n",
        "Configures LoRA (Low-Rank Adaptation) for efficient fine-tuning."
      ],
      "metadata": {
        "id": "jCErpTT8khZ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step9: Setup/Apply LoRA finetuning to the model\n",
        "\n",
        "model_lora = FastLanguageModel.get_peft_model(\n",
        "    model = model,\n",
        "    r = 16,\n",
        "    target_modules = [\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\"\n",
        "    ],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3047,\n",
        "    use_rslora = False,\n",
        "    loftq_config = None\n",
        ")"
      ],
      "metadata": {
        "id": "AuHDDG5hKRli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clear Model Attribute**\n",
        "\n",
        "Removes a potential conflicting attribute from the model"
      ],
      "metadata": {
        "id": "uGEbwAlOkpxa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add this before creating the trainer\n",
        "if hasattr(model, '_unwrapped_old_generate'):\n",
        "    del model._unwrapped_old_generate"
      ],
      "metadata": {
        "id": "KMCuLCXpKX77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Set Up Fine-Tuning Trainer**\n",
        "\n",
        "Configures the SFTTrainer for fine-tuning."
      ],
      "metadata": {
        "id": "ohoLOOiak00L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model = model_lora,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = finetune_dataset,\n",
        "    dataset_text_field = \"texts\",\n",
        "    max_seq_length = max_sequence_length,\n",
        "    dataset_num_proc = 1,\n",
        "\n",
        "    # Define training args\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        num_train_epochs = 1,\n",
        "        warmup_steps = 5,\n",
        "        max_steps = 60,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16=is_bfloat16_supported(),\n",
        "        logging_steps = 10,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=3407,\n",
        "        output_dir = \"outputs\",\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "7LwpYeQ-KX-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Set Up Weights & Biases (W&B)**\n",
        "\n",
        "Configures W&B for training monitoring."
      ],
      "metadata": {
        "id": "y3s1lUZMk9S7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup WANDB\n",
        "from google.colab import userdata\n",
        "wnb_token = userdata.get(\"WANDB_TO_TOKEN\")\n",
        "# Login to WnB\n",
        "wandb.login(key=wnb_token) # import wandb\n",
        "run = wandb.init(\n",
        "    project='Fine-tune-DeepSeek-R1-on-Medical-CoT-Dataset',\n",
        "    job_type=\"training\",\n",
        "    anonymous=\"allow\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "BMOEgeSkKNH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Start Fine-Tuning**\n",
        "\n",
        "Runs the fine-tuning process."
      ],
      "metadata": {
        "id": "Hwzn9SadlE64"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Start the fine-tuning process\n",
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "id": "p0YCdm9nKhhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Finish W&B Run**\n",
        "\n",
        "Closes the W&B session."
      ],
      "metadata": {
        "id": "i0_fmecFlQ_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.finish()"
      ],
      "metadata": {
        "id": "ArlhoORCKssS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test Fine-Tuned Model (Cystometry)**\n",
        "\n",
        "Tests the fine-tuned model with the cystometry question."
      ],
      "metadata": {
        "id": "TPnlHqdTlVrj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step10: Testing after fine-tuning\n",
        "question = \"\"\"A 61-year-old woman with a long history of involuntary urine loss during activities like coughing or sneezing\n",
        "              but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings,\n",
        "              what would cystometry most likely reveal about her residual volume and detrusor contractions?\"\"\"\n",
        "\n",
        "FastLanguageModel.for_inference(model_lora)\n",
        "\n",
        "# Tokenize the input\n",
        "inputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# Generate a response\n",
        "outputs = model_lora.generate (\n",
        "    input_ids = inputs.input_ids,\n",
        "    attention_mask = inputs.attention_mask,\n",
        "    max_new_tokens = 1200,\n",
        "    use_cache = True\n",
        ")\n",
        "\n",
        "# Decode the response tokens back to text\n",
        "response = tokenizer.batch_decode(outputs)\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "id": "OVYErAzhKhmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response[0].split(\"### Answer:\")[1])"
      ],
      "metadata": {
        "id": "FToI2VLGKyYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test Fine-Tuned Model (Aortic Valve)**\n",
        "\n",
        "Tests the fine-tuned model with a new question about aortic valve vegetation."
      ],
      "metadata": {
        "id": "G0JoeZ0Plpx7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"\"\"A 59-year-old man presents with a fever, chills, night sweats, and generalized fatigue,\n",
        "              and is found to have a 12 mm vegetation on the aortic valve. Blood cultures indicate gram-positive, catalase-negative,\n",
        "              gamma-hemolytic cocci in chains that do not grow in a 6.5% NaCl medium.\n",
        "              What is the most likely predisposing factor for this patient's condition?\"\"\"\n",
        "\n",
        "FastLanguageModel.for_inference(model_lora)\n",
        "\n",
        "# Tokenize the input\n",
        "inputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# Generate a response\n",
        "outputs = model_lora.generate (\n",
        "    input_ids = inputs.input_ids,\n",
        "    attention_mask = inputs.attention_mask,\n",
        "    max_new_tokens = 1200,\n",
        "    use_cache = True\n",
        ")\n",
        "\n",
        "# Decode the response tokens back to text\n",
        "response = tokenizer.batch_decode(outputs)\n",
        "\n",
        "print(response[0].split(\"### Answer:\")[1])"
      ],
      "metadata": {
        "id": "qk6kxR7vK3U2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**User Interface**\n",
        "\n",
        " Developed a user-friendly interface with Gradio, allowing seamless input of health prompts and display of informative responses,\n",
        " enhancing accessibility for non-technical users."
      ],
      "metadata": {
        "id": "xCfcVxigl49b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 1: Install Gradio (skip if already installed)\n",
        "print(\"Installing Gradio...\")\n",
        "!pip install gradio --quiet\n",
        "print(\"Gradio installed.\")\n",
        "\n",
        "# Step 2: Import Gradio\n",
        "try:\n",
        "    import gradio as gr\n",
        "except ImportError as e:\n",
        "    raise ImportError(f\"Failed to import Gradio: {e}. Please ensure Gradio is installed.\")\n",
        "\n",
        "# Step 3: Placeholder chatbot function (no model loading)\n",
        "def ai_doctor_chatbot(question):\n",
        "    if not question.strip():\n",
        "        return \"Please enter a medical question.\"\n",
        "\n",
        "    # Placeholder response (replace with model inference after fixing notebook)\n",
        "    placeholder_response = (\n",
        "        \"This is a placeholder response because the fine-tuned model is not available. \"\n",
        "        \"To enable AI responses, please fix AI_Doctor_3.ipynb by updating Cell 7 with a valid model \"\n",
        "        \"(e.g., 'mistralai/Mixtral-8x7B-Instruct-v0.1'), re-run the notebook to generate the 'outputs' directory, \"\n",
        "        \"and then use the full Gradio script with model loading.\"\n",
        "    )\n",
        "\n",
        "    return f\"**Disclaimer**: This is an AI-generated response. Consult a doctor for professional medical advice.\\n\\n**Response**:\\n{placeholder_response}\"\n",
        "\n",
        "# Step 4: Create minimal Gradio interface\n",
        "interface = gr.Interface(\n",
        "    fn=ai_doctor_chatbot,\n",
        "    inputs=gr.Textbox(\n",
        "        label=\"Ask a Medical Question\",\n",
        "        placeholder=\"E.g., What would cystometry reveal for stress urinary incontinence?\"\n",
        "    ),\n",
        "    outputs=gr.Textbox(label=\"Medichat Response\"),\n",
        "    title=\" MediChat: AI Health Assistant Chatbot\",\n",
        "    description=\"Get instant, reliable answers to your health-related questions. MediChat uses AI to provide supportive information based on real medical conversations.\",\n",
        "    theme=\"soft\",  # Clean, Grok-like aesthetic\n",
        "    allow_flagging=\"never\"  # Disable flagging for simplicity\n",
        ")\n",
        "\n",
        "# Step 5: Launch the interface\n",
        "print(\"Launching Gradio interface...\")\n",
        "interface.launch()\n"
      ],
      "metadata": {
        "id": "sxU-hKZ2KyN6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}